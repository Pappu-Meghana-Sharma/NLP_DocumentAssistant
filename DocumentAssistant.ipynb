{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Qa39hSeBD64"
   },
   "source": [
    "## Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "FvRPeIlAyg7H"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "9LhTmBDg0CpS"
   },
   "outputs": [],
   "source": [
    "telugu_sentences = [\n",
    "    \"రాజు పుస్తకం చదువుతున్నాడు\",\n",
    "    \"సూర్యుడు ఉదయమవుతోంది\",\n",
    "    \"పిల్లలు పార్క్ లో ఆడుతున్నారు\",\n",
    "    \"రాము మరియు నందిని సముద్ర తీరంలో తిరుగుతున్నారు\",\n",
    "    \"ఆమె వంటగదిలో కుక్కీ తయారు చేస్తోంది\",\n",
    "    \"సైనికులు తమ విధులు నిబద్ధతగా చేస్తున్నారు\",\n",
    "    \"బెంగళూరు లో కొత్త విద్యుత్ ప్రాజెక్ట్ ప్రారంభమైంది\",\n",
    "    \"అతను నేటి వార్తల గురించి చదువుతున్నాడు\",\n",
    "    \"వర్షం కారణంగా ఆట రద్దు అయ్యింది\",\n",
    "    \"రాజు మరియు రాము బ్లాక్ లో ఆడుతున్నారు\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "id": "Y4w5zniv0GC0"
   },
   "outputs": [],
   "source": [
    "def tokenize_telugu_sentence(sentence):\n",
    "    return [w for w in sentence.split() if re.match(r'[\\u0C00-\\u0C7F]+', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLdUp5DS0-f-",
    "outputId": "f815e11d-3a03-4405-d575-17629480a5f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['రాజు', 'పుస్తకం', 'చదువుతున్నాడు']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_telugu_sentence(telugu_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "id": "5nU-MJSs5j5_"
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [tokenize_telugu_sentence(s) for s in telugu_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LH_e9q0g5JNX",
    "outputId": "05c396d1-9dd9-491f-d90a-6c486b2b4e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 tokens: ['రాజు', 'పుస్తకం', 'చదువుతున్నాడు']\n",
      "Sentence 2 tokens: ['సూర్యుడు', 'ఉదయమవుతోంది']\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(tokenized_sentences[:2]):\n",
    "    print(f\"Sentence {i+1} tokens:\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "V6X76ci85icK"
   },
   "outputs": [],
   "source": [
    "bigram_counts = defaultdict(Counter)\n",
    "\n",
    "for tokens in tokenized_sentences:\n",
    "    tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    for i in range(len(tokens)-1):\n",
    "        bigram_counts[tokens[i]][tokens[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de6rzYvP5qNi",
    "outputId": "853c3146-9002-4659-a4fc-9c6d6885b939"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('రాజు', 2), ('సూర్యుడు', 1), ('పిల్లలు', 1)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counts[\"<s>\"].most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "_-U7T0uA5uNo"
   },
   "outputs": [],
   "source": [
    "trigram_counts = defaultdict(Counter)\n",
    "for tokens in tokenized_sentences:\n",
    "    tokens = [\"<s>\", \"<s>\"] + tokens + [\"</s>\"]\n",
    "    for i in range(len(tokens)-2):\n",
    "        context = (tokens[i], tokens[i+1])\n",
    "        trigram_counts[context][tokens[i+2]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLbDWt1a6bEP",
    "outputId": "db92d872-d2a3-4e45-bfc4-c57663f50a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'రాజు': 2,\n",
       "         'సూర్యుడు': 1,\n",
       "         'పిల్లలు': 1,\n",
       "         'రాము': 1,\n",
       "         'ఆమె': 1,\n",
       "         'సైనికులు': 1,\n",
       "         'బెంగళూరు': 1,\n",
       "         'అతను': 1,\n",
       "         'వర్షం': 1})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_counts[('<s>','<s>')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wnfS0_FA500F",
    "outputId": "60268b8a-4e39-435f-c8ae-cd55fe79d927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next words for context ('రాము', 'బ్లాక్') : [('లో', 1)]\n"
     ]
    }
   ],
   "source": [
    "context = (\"రాము\", \"బ్లాక్\")\n",
    "print(\"Next words for context\", context, \":\", trigram_counts[context].most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "bkMTIsEL53R0"
   },
   "outputs": [],
   "source": [
    "def predict_next_word(context, trigram_counts, top_k=3):\n",
    "    candidates = trigram_counts.get(context, {})\n",
    "    total = sum(candidates.values())\n",
    "    if not total:\n",
    "        return []\n",
    "    probs = {w: c/total for w,c in candidates.items()}\n",
    "    return sorted(probs.items(), key=lambda x: x[1], reverse=True)[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UTXsVSC60ku",
    "outputId": "45ee551c-d47f-421f-b907-d44ee0653892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('లో', 1.0)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_next_word((\"రాము\", \"బ్లాక్\"), trigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "_DsY88pm606T"
   },
   "outputs": [],
   "source": [
    "def compute_perplexity(tokenized_sentences, trigram_counts, vocab_size):\n",
    "    N = 0\n",
    "    log_prob_sum = 0\n",
    "    for tokens in tokenized_sentences:\n",
    "        tokens = [\"<s>\", \"<s>\"] + tokens + [\"</s>\"]\n",
    "        for i in range(2, len(tokens)):\n",
    "            context = (tokens[i-2], tokens[i-1])\n",
    "            word = tokens[i]\n",
    "            count_context = sum(trigram_counts.get(context, {}).values())\n",
    "            word_count = trigram_counts.get(context, {}).get(word, 0)\n",
    "            prob = (word_count + 1)/(count_context + vocab_size)\n",
    "            log_prob_sum += -math.log(prob)\n",
    "            N += 1\n",
    "    return math.exp(log_prob_sum/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCCz7ox87EKS",
    "outputId": "578e25b2-9398-45ca-e373-0d5c1342ea46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.640217955836274"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(w for s in tokenized_sentences for w in s) | {\"<s>\", \"</s>\"}\n",
    "compute_perplexity(tokenized_sentences, trigram_counts, len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "-cuXf9hK7GdO"
   },
   "outputs": [],
   "source": [
    "class NGramModel:\n",
    "    def __init__(self, n, lambda_trigram=0.6, lambda_bigram=0.3, lambda_unigram=0.1):\n",
    "        self.n = n\n",
    "        self.ngrams = defaultdict(Counter)\n",
    "        self.vocab = set()\n",
    "        self.unigram_counts = Counter()\n",
    "        self.lambda1 = lambda_trigram\n",
    "        self.lambda2 = lambda_bigram\n",
    "        self.lambda3 = lambda_unigram\n",
    "\n",
    "        total = self.lambda1 + self.lambda2 + self.lambda3\n",
    "        if abs(total - 1) > 0.001:\n",
    "            self.lambda1 = 0.6\n",
    "            self.lambda2 = 0.3\n",
    "            self.lambda3 = 0.1\n",
    "    def train(self, tokenized_sentences):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.vocab = set()\n",
    "\n",
    "        if tokenized_sentences and isinstance(tokenized_sentences[0], str):\n",
    "            tokenized_sentences = [self.tokenize_telugu_sentence(s) for s in tokenized_sentences]\n",
    "\n",
    "\n",
    "        for tokens in tokenized_sentences:\n",
    "            for word in tokens:\n",
    "                self.unigram_counts[word] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "\n",
    "        for tokens in tokenized_sentences:\n",
    "            tokens = [\"<s>\"] * (self.n - 1) + tokens + [\"</s>\"]\n",
    "            self.vocab.update(tokens)\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i+self.n-1])\n",
    "                target = tokens[i+self.n-1]\n",
    "                self.ngrams[context][target] += 1\n",
    "\n",
    "    def tokenize_telugu_sentence(self, sentence):\n",
    "        return [w for w in sentence.split() if re.match(r'[\\u0C00-\\u0C7F]+', w)]\n",
    "\n",
    "    def predict_next(self, context, top_k=3):\n",
    "        all_predictions = {}\n",
    "        unigram_total = sum(self.unigram_counts.values())\n",
    "        \n",
    "        candidate_words = set()\n",
    "        if len(context) >= self.n - 1:\n",
    "            ngram_context = tuple(context[-(self.n-1):])\n",
    "            ngram_data = self.ngrams.get(ngram_context, {})\n",
    "            ngram_total = sum(ngram_data.values())\n",
    "            \n",
    "            if ngram_total > 0:\n",
    "                for word, count in ngram_data.items():\n",
    "                    if word not in ['<s>', '</s>']:\n",
    "                        ngram_prob = count / ngram_total\n",
    "                        all_predictions[word] = self.lambda1 * ngram_prob\n",
    "                        candidate_words.add(word)\n",
    "\n",
    "        for word, count in self.unigram_counts.most_common(100):\n",
    "            if word not in ['<s>', '</s>'] and word not in candidate_words:\n",
    "                unigram_prob = count / unigram_total\n",
    "                all_predictions[word] = self.lambda3 * unigram_prob\n",
    "        \n",
    "        if all_predictions:\n",
    "            total_prob = sum(all_predictions.values())\n",
    "            if total_prob > 0:\n",
    "                normalized_predictions = {word: prob/total_prob for word, prob in all_predictions.items()}\n",
    "                sorted_predictions = sorted(normalized_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "                return sorted_predictions[:top_k]\n",
    "        \n",
    "        return []\n",
    "\n",
    "\n",
    "    def perplexity(self, test_sentences):\n",
    "        if test_sentences and isinstance(test_sentences[0], str):\n",
    "            test_sentences = [self.tokenize_telugu_sentence(s) for s in test_sentences]\n",
    "\n",
    "        N = 0\n",
    "        log_prob_sum = 0\n",
    "        unigram_total = sum(self.unigram_counts.values())\n",
    "\n",
    "        for tokens in test_sentences:\n",
    "            tokens = [\"<s>\"] * (self.n - 1) + tokens + [\"</s>\"]\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tuple(tokens[i - self.n + 1:i])\n",
    "                word = tokens[i]\n",
    "                ngram_prob = 0\n",
    "                ngram_data = self.ngrams.get(context, {})\n",
    "                ngram_total = sum(ngram_data.values())\n",
    "                if ngram_total > 0:\n",
    "                    ngram_prob = ngram_data.get(word, 0) / ngram_total\n",
    "                unigram_prob = self.unigram_counts.get(word, 0) / unigram_total\n",
    "                prob = max(self.lambda1 * ngram_prob + self.lambda3 * unigram_prob, 1e-10)\n",
    "                log_prob_sum += -math.log(prob)\n",
    "                N += 1\n",
    "\n",
    "        return math.exp(log_prob_sum / N) if N > 0 else float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "2SLIH9yH7TJS"
   },
   "outputs": [],
   "source": [
    "model = NGramModel(n=3)\n",
    "model.train(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_sentences))\n",
    "print(sum(len(s) for s in tokenized_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "id": "VUNPNBA1_kvN"
   },
   "outputs": [],
   "source": [
    "\n",
    "phrases = [\n",
    "    [\"రాజు\"],\n",
    "    [\"పిల్లలు\", \"పార్క్\"],\n",
    "    [\"రాము\", \"మరియు\"],\n",
    "    [\"ఆమె\", \"వంటగదిలో\"],\n",
    "    [\"బెంగళూరు\", \"లో\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgcQ4tTk_nWi",
    "outputId": "e1659ba4-4a41-4f3f-f29e-1f6a9ba63275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: రాజు\n",
      "లో:(0.06)\n",
      "రాజు:(0.04)\n",
      "చదువుతున్నాడు:(0.04)\n",
      "\n",
      "Input: పిల్లలు పార్క్\n",
      "లో:(0.87)\n",
      "రాజు:(0.01)\n",
      "చదువుతున్నాడు:(0.01)\n",
      "\n",
      "Input: రాము మరియు\n",
      "నందిని:(0.86)\n",
      "లో:(0.01)\n",
      "రాజు:(0.01)\n",
      "\n",
      "Input: ఆమె వంటగదిలో\n",
      "కుక్కీ:(0.86)\n",
      "లో:(0.01)\n",
      "రాజు:(0.01)\n",
      "\n",
      "Input: బెంగళూరు లో\n",
      "కొత్త:(0.86)\n",
      "లో:(0.01)\n",
      "రాజు:(0.01)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4714303982860715"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for phrase in phrases:\n",
    "    preds = model.predict_next(phrase)\n",
    "    print(\"Input:\", \" \".join(phrase))\n",
    "    if preds:\n",
    "        for word, p in preds:\n",
    "            print(f\"{word}:({p:.2f})\")\n",
    "    else:\n",
    "        print(\"No prediction found\")\n",
    "    print()\n",
    "\n",
    "model.perplexity(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6svCyUOIBMc_"
   },
   "source": [
    "## Sentence Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "HvlFV2Q-BRkR"
   },
   "outputs": [],
   "source": [
    "def complete_sentence(model, context_list, max_len=20):\n",
    "    completed = context_list.copy()\n",
    "    for _ in range(max_len):\n",
    "        if len(completed) < model.n - 1:\n",
    "            current_context = [\"<s>\"] * (model.n - 1 - len(completed)) + completed\n",
    "        else:\n",
    "            current_context = completed[-(model.n-1):]\n",
    "\n",
    "        context_tuple = tuple(current_context)\n",
    "        candidates = model.ngrams.get(context_tuple, {})\n",
    "        if not candidates:\n",
    "            if len(completed) > 0:\n",
    "                context_bi = tuple(completed[-1:])\n",
    "                candidates = model.ngrams.get(context_bi, {})\n",
    "        if not candidates:\n",
    "            break\n",
    "        total = sum(candidates.values())\n",
    "        probs = {w: c/total for w, c in candidates.items()}\n",
    "        next_word = max(probs, key=probs.get)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        completed.append(next_word)\n",
    "    return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XrDutq9UBRmY",
    "outputId": "48f35076-1804-4b71-81c8-93ab246b1720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "రాజు పుస్తకం చదువుతున్నాడు\n"
     ]
    }
   ],
   "source": [
    "context= [\"రాజు\"]\n",
    "model=NGramModel(3)\n",
    "model.train(tokenized_sentences)\n",
    "completed_sentence = complete_sentence(model, context)\n",
    "print(\" \".join(completed_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI-f4n-IAv0Q"
   },
   "source": [
    "## Levishtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "BRlvHjvZ_yhD"
   },
   "outputs": [],
   "source": [
    "def levenshtein_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m+1): dp[i][0] = i\n",
    "    for j in range(n+1): dp[0][j] = j\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rDdt-zp6A0sn",
    "outputId": "ac42b045-82b9-4808-e96f-bc60638bd9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'రాజు': 0, 'రాజి': 1, 'రాజ': 1}\n"
     ]
    }
   ],
   "source": [
    "word = \"రాజు\"\n",
    "candidates = [\"రాజు\", \"రాజి\", \"రాజ\"]\n",
    "distances = {w: levenshtein_distance(word, w) for w in candidates}\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "0c-uZR-dMFzz"
   },
   "outputs": [],
   "source": [
    "class TeluguSpellChecker:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "    def levenshtein_distance(self, s1, s2):\n",
    "        m, n = len(s1), len(s2)\n",
    "        dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "        for i in range(m+1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(n+1):\n",
    "            dp[0][j] = j\n",
    "        for i in range(1, m+1):\n",
    "            for j in range(1, n+1):\n",
    "                if s1[i-1] == s2[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "        return dp[m][n]\n",
    "\n",
    "    def suggest_corrections(self, word, max_suggestions=3):\n",
    "        if word in self.vocabulary:\n",
    "            return []\n",
    "        suggestions = []\n",
    "        for correct_word in self.vocabulary:\n",
    "            dist = self.levenshtein_distance(word, correct_word)\n",
    "            if dist <= 2:\n",
    "                suggestions.append((correct_word,dist))\n",
    "        suggestions.sort(key=lambda x: x[1])\n",
    "        return [word for word, dist in suggestions[:max_suggestions]]\n",
    "\n",
    "    def check_sentence(self, sentence):\n",
    "        if isinstance(sentence, str):\n",
    "            words = [w for w in sentence.split() if re.match(r'[\\u0C00-\\u0C7F]+', w)]\n",
    "        else:\n",
    "            words=sentence\n",
    "\n",
    "        corrections = {}\n",
    "        for word in sentence.split():\n",
    "            suggestions = self.suggest_corrections(word)\n",
    "            if suggestions:\n",
    "                corrections[word] = suggestions\n",
    "        return corrections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKNfbaPJA39N"
   },
   "source": [
    "## Code Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OqAzvf5sexRZ",
    "outputId": "fde04ed1-9438-431e-85c4-da8ec8d5df6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "రాజు: ['లో', 'రాజు', 'చదువుతున్నాడు']\n",
      "పిల్లలు పార్క్: ['లో', 'రాజు', 'చదువుతున్నాడు']\n",
      "రాము మరియు: ['నందిని', 'లో', 'రాజు']\n",
      "Completed Sentence: రాము మరియు నందిని సముద్ర తీరంలో తిరుగుతున్నారు\n",
      "Sentence రాజు పుస్తకం చదువుతున్నాడు is already correct\n",
      "Actual Sentence: రాజి పుస్తక చదువుతున్నాడు\n",
      "Corrected_sentence: {'రాజి': ['రాజు', 'రాము'], 'పుస్తక': ['పుస్తకం']}\n",
      "Sentence పిల్లలు పార్క్ లో ఆడుతున్నారు is already correct\n",
      "Perplexity: 2.47\n"
     ]
    }
   ],
   "source": [
    "model = NGramModel(n=3)\n",
    "model.train(tokenized_sentences)\n",
    "phrases = [[\"రాజు\"], [\"పిల్లలు\", \"పార్క్\"], [\"రాము\", \"మరియు\"]]\n",
    "for phrase in phrases:\n",
    "    preds = model.predict_next(phrase)\n",
    "    print(f\"{' '.join(phrase)}: {[p[0] for p in preds]}\")\n",
    "\n",
    "vocab = model.vocab   \n",
    "completed = complete_sentence(model, [\"రాము\"])\n",
    "print(f\"Completed Sentence: {' '.join(completed)}\")\n",
    "spell_checker = TeluguSpellChecker(vocab)\n",
    "test_sentences = [\n",
    "        \"రాజు పుస్తకం చదువుతున్నాడు\",\n",
    "        \"రాజి పుస్తక చదువుతున్నాడు\",\n",
    "        \"పిల్లలు పార్క్ లో ఆడుతున్నారు\"\n",
    "    ]\n",
    "for sentence in test_sentences:\n",
    "    corrections=spell_checker.check_sentence(sentence)\n",
    "    if corrections:\n",
    "        print(f\"Actual Sentence: {sentence}\")\n",
    "        print(f\"Corrected_sentence: {corrections}\")\n",
    "    else:\n",
    "        print(f\"Sentence {sentence} is already correct\")\n",
    "\n",
    "print(f\"Perplexity: {model.perplexity(tokenized_sentences):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BpQTIyN_g-vQ",
    "outputId": "ddb6a947-73d9-414e-d85f-482d291d67e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-tokenized JSON not available and exited with error Invalid control character at: line 3 column 7 (char 12)\n",
      "Loaded 179132 sentences from cleaned data\n",
      "Tokenized 179132 sentences\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import json\n",
    "#rewrite this function if json works..remove other part of function accordingly \n",
    "def load_telugu_data():\n",
    "    sentences = []\n",
    "    try:\n",
    "        with open('telugu_tokenized_sentences.json', 'r', encoding='utf-8') as f:       #if json works correctly, function returns \n",
    "            tokenized_sentences = json.load(f)\n",
    "        print(f\"Loaded {len(tokenized_sentences)} pre-tokenized sentences from JSON\")\n",
    "        return tokenized_sentences\n",
    "    except Exception as e:\n",
    "        print(f\"Pre-tokenized JSON not available and exited with error {e}\")\n",
    "\n",
    "    try:\n",
    "        with open('Data/final_cleaned_telugu_data.txt', 'r', encoding='utf-8') as f:             #directly strip sentences from cleaned data\n",
    "            sentences = [line.strip() for line in f if line.strip()]\n",
    "        print(f\"Loaded {len(sentences)} sentences from cleaned data\")\n",
    "    except:\n",
    "        print(\"Cleaned data file not available\")\n",
    "\n",
    "    if not sentences:                                                                      #if not, load the data from telugu_sentences\n",
    "        try:\n",
    "            with open('telugu_sentences.txt', 'r', encoding='utf-8') as f:\n",
    "                sentences = [line.strip() for line in f if line.strip()]\n",
    "            print(f\"Loaded {len(sentences)} sentences from sentences file\")\n",
    "        except :\n",
    "            print(f\"Sentences file not available, using sample data\")\n",
    "            sentences = [\n",
    "                \"రాజు పుస్తకం చదువుతున్నాడు\",\n",
    "                \"సూర్యుడు ఉదయమవుతోంది\",\n",
    "                \"పిల్లలు పార్క్ లో ఆడుతున్నారు\",\n",
    "                \"రాము మరియు నందిని సముద్ర తీరంలో తిరుగుతున్నారు\",\n",
    "                \"ఆమె వంటగదిలో కుక్కీ తయారు చేస్తోంది\"\n",
    "            ]\n",
    "\n",
    "    if sentences and isinstance(sentences[0], str):\n",
    "        tokenized_sentences = [tokenize_telugu_sentence(s) for s in sentences]\n",
    "        print(f\"Tokenized {len(tokenized_sentences)} sentences\")\n",
    "        return tokenized_sentences\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def load_telugu_vocabulary():\n",
    "    try:\n",
    "        with open('telugu_vocabulary.txt', 'r', encoding='utf-8') as f:\n",
    "            vocabulary = set(line.strip() for line in f if line.strip())\n",
    "        print(f\"Loaded {len(vocabulary)} words from vocabulary file\")\n",
    "        return vocabulary\n",
    "    except:\n",
    "        print(\"Vocabulary file not available, will use model vocabulary\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "tokenized_sentences = load_telugu_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ngram_model(model, filename='telugu_ngram_model.pkl'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_ngram_model(filename='telugu_ngram_model.pkl'):\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model\n",
    "    except:\n",
    "        print(f\"Could not load model from {filename}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "6tlGorhyFP38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  భారత దేశం: ['మొత్తం', 'నుండి', 'అవుతుంది.']\n",
      "  తెలుగు భాష: ['మాట్లాడే', 'చరిత్ర', 'ఈ']\n",
      "  హైదరాబాద్ లో: ['ఉండగా', 'అమీర్', 'జీవిస్తున్నారు.']\n",
      "\n",
      "PERFORMANCE METRICS:\n",
      "  Vocabulary size: 276347 words\n",
      "  Training sentences: 179132\n",
      "  Perplexity: 6.23\n"
     ]
    }
   ],
   "source": [
    "model = NGramModel(n=3)\n",
    "model.train(tokenized_sentences)\n",
    "\n",
    "\n",
    "wiki_phrases = [\n",
    "    [\"భారత\", \"దేశం\"],\n",
    "    [\"తెలుగు\", \"భాష\"], \n",
    "    [\"హైదరాబాద్\", \"లో\"]\n",
    "]\n",
    "\n",
    "for phrase in wiki_phrases:\n",
    "    preds = model.predict_next(phrase, top_k=3)\n",
    "    print(f\"  {' '.join(phrase)}: {[p[0] for p in preds]}\")\n",
    "    \n",
    "\n",
    "print(f\"\\nPERFORMANCE METRICS:\")\n",
    "print(f\"  Vocabulary size: {len(model.vocab)} words\")\n",
    "print(f\"  Training sentences: {len(tokenized_sentences)}\")\n",
    "print(f\"  Perplexity: {model.perplexity(tokenized_sentences):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to telugu_ngram_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#save_ngram_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from telugu_ngram_model.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = load_ngram_model()\n",
    "\n",
    "if model is None:\n",
    "    model = NGramModel(n=3)\n",
    "    model.train(tokenized_sentences)\n",
    "    save_ngram_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Completion:\n",
      "Completed Sentence: రాజు తన పాలనను దైవిక మూలం నుండి దిగడం ద్వారా కాకుండా, బౌద్ధ సంఘం ఆమోదం పొందడం ద్వారా సంపాదించడానికి ప్రయత్నించాడు.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence Completion:\")\n",
    "completed = complete_sentence(model, [\"రాజు\"])\n",
    "print(f\"Completed Sentence: {' '.join(completed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
